{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "/Users/gianlucascarpellini/dev/vipm-project\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "%cd ..\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from matcher.features import FeatureMatcher\n",
    "from torch.utils.data import DataLoader\n",
    "from matcher.dataset import ClassificationDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "config = {\n",
    "    'data_path': 'data/fashion-product-images-small/images',\n",
    "    'exp_base_dir': 'data/exps/exp1',\n",
    "    'image_size': (224, 224),\n",
    "    'load_path': \"data/models/resnet18_best.pt\",\n",
    "    'features_path': 'data/features/featuresresnet18.npy',\n",
    "    'index_path': 'data/features/featuresresnet18_index.pickle',\n",
    "    'segmentation_path': 'data/models/segm.pth',\n",
    "    \"classes\": [\"masterCategory\", \"subCategory\", \"gender\"],\n",
    "    \"segmentation\":False\n",
    "\n",
    "}\n",
    "# [\"gender\", \"subCategory\", \"masterCategory\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Loading model\n",
      "Loaded in 33.04561901092529s\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "fm = FeatureMatcher(features_path=config['features_path'], model_path=config['load_path'],\n",
    "                    index_path=config['index_path'],\n",
    "                    segmentation_model_path=config['segmentation_path'])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Loading\n",
      "     masterCategory subCategory gender\n0          Footwear       Shoes    Men\n1       Accessories       Belts  Women\n2       Accessories        Bags  Women\n3       Accessories        Bags  Women\n4          Footwear      Sandal    Men\n...             ...         ...    ...\n8880    Accessories       Belts    Men\n8881    Accessories     Watches  Women\n8882    Accessories     Watches    Men\n8883    Accessories       Belts  Women\n8884       Footwear       Shoes    Men\n\n[8819 rows x 3 columns]\n       masterCategory subCategory gender\ncount            8819        8819   8819\nunique              7          39      5\ntop           Apparel     Topwear    Men\nfreq             4239        3082   4436\nmasterCategory  subCategory       gender\nAccessories     Accessories       Men        20\n                                  Unisex      8\n                Bags              Men        19\n                                  Unisex    204\n                                  Women     422\n                                           ... \nPersonal Care   Nails             Women      50\n                Skin              Women       6\n                Skin Care         Men         1\n                                  Women       8\nSporting Goods  Sports Equipment  Unisex      5\nName: id, Length: 95, dtype: int64\nBuilding classification dataset \n(8819, 3)\n[[ 2. 28.  2.]\n [ 0.  4.  4.]\n [ 0.  2.  4.]\n ...\n [ 0. 37.  2.]\n [ 0.  4.  4.]\n [ 2. 28.  2.]] [7, 39, 5]\nBuilding dataset -- Done\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(\"Loading\")\n",
    "test_loader = DataLoader(\n",
    "    ClassificationDataset(\n",
    "        \"./data/fashion-product-images-small/images\",\n",
    "        \"./data/small_test.csv\",\n",
    "        distinguish_class=config[\"classes\"],\n",
    "        image_size=config[\"image_size\"],\n",
    "        transform=transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "        thr=5,\n",
    "    ),\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-66217072ce31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegment_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             output = fm.get_k_most_similar(data[0], similar_type=similarity,\n\u001b[0;32m---> 16\u001b[0;31m                                            image_size=config[\"image_size\"])\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/vipm-project/matcher/features.py\u001b[0m in \u001b[0;36mget_k_most_similar\u001b[0;34m(self, image, image_size, k, device, similar_type, net_name)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     def get_k_most_similar(self, x, image_size, k=1, device=\"cpu\", similar_type=0,\n\u001b[0m\u001b[1;32m     83\u001b[0m                            net_name=\"resnet\"):\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/vipm-project-j9kWruqw/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, *sizes)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"non-inplace resize is deprecated\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mResize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mresize_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/vipm-project-j9kWruqw/lib/python3.7/site-packages/torch/autograd/_functions/tensor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, tensor, sizes)\u001b[0m\n\u001b[1;32m     35\u001b[0m                                 \u001b[0;34m\"tensor, while preserving the number of elements. \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0;34m'x'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 'x'.join(map(str, tensor.size())), tensor.numel()))\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_contiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: requested resize to (224, 224) ((224, 224) elements in total), but the given tensor has a size of 3x224x224 (150528 elements). autograd's resize can only change the shape of a given tensor, while preserving the number of elements. "
     ],
     "ename": "RuntimeError",
     "evalue": "requested resize to (224, 224) ((224, 224) elements in total), but the given tensor has a size of 3x224x224 (150528 elements). autograd's resize can only change the shape of a given tensor, while preserving the number of elements. ",
     "output_type": "error"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "with torch.no_grad():\n",
    "    accurate_labels = 0\n",
    "    all_labels = 0\n",
    "    val_loss = []\n",
    "    accurate_labels = [0, 0, 0]\n",
    "    accuracies = [0, 0, 0]\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        for i in range(len(data)):\n",
    "            data[i] = data[i].to(device)\n",
    "            \n",
    "        for similarity in range(0, 4): # Per ogni classe di similarit√†\n",
    "            if config[\"segmentation\"]:\n",
    "                image = fm.segment_image(image)\n",
    "            output = fm.get_k_most_similar(data[0], similar_type=similarity,\n",
    "                                           image_size=config[\"image_size\"])\n",
    "            target = target.long()\n",
    "    \n",
    "\n",
    "        all_labels += len(target)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    for i in range(n_label):\n",
    "        accuracies[i] = 100.0 * accurate_labels[i].item() / all_labels\n",
    "    print(\n",
    "        \"Test accuracy: ({})/{} ({}), Loss: ({})\".format(\n",
    "            \", \".join([str(accurate_labels[i].item()) for i in range(n_label)]),\n",
    "            all_labels,\n",
    "            \", \".join([\"{:.3f}%\".format(accuracies[i]) for i in range(n_label)]),\n",
    "            \", \".join(\n",
    "                \"{:.6f}\".format(loss)\n",
    "                for loss in torch.mean(torch.tensor(val_loss), dim=0).data.tolist()\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "image = Image.open(local_filename).convert(\"RGB\")\n",
    "if segmentation:\n",
    "    image = fm.segment_image(image)\n",
    "classes = fm.classify(image, image_size=image_size)\n",
    "\n",
    "labels = [f\"In base al tuo sesso {classes[0]}\",\n",
    "          f\"In base al tipo di abito {classes[1]}\",\n",
    "          f\"In base al sottotipo di abito {classes[2]}\",\n",
    "          f\"Simili al tuo stile\"]\n",
    "\n",
    "for i in range(0, 4):\n",
    "    bot.sendMessage(chat_id, labels[i])\n",
    "    result = fm.get_k_most_similar(image, image_size=image_size,\n",
    "                                   k=k, similar_type=i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}